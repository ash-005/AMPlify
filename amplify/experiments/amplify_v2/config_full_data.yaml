# Full training set with imbalanced non-AMP data
model:
  backbone: "lstm"
  pooling: "context"
  input_dim: 20
  d_model: 256
  num_layers: 3
  max_len: 200

training:
  batch_size: 128
  epochs: 25
  lr: 2e-4
  seed: 50
  limit_per_class: null  # Use ALL available data

logging:
  save_dir: "amplify/experiments/logs"
  save_checkpoints: true

optional:
  use_esm: false
  esm_dim: null
