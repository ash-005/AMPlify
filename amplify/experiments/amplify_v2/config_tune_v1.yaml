# Tuning v1: longer training, higher LR, more data, BiLSTM backbone
model:
  backbone: "lstm"
  pooling: "context"
  input_dim: 20
  d_model: 256
  num_layers: 2
  max_len: 200

training:
  batch_size: 64
  epochs: 20
  lr: 3e-4
  seed: 42
  limit_per_class: 2000

logging:
  save_dir: "amplify/experiments/logs"
  save_checkpoints: true

optional:
  use_esm: false
  esm_dim: null
