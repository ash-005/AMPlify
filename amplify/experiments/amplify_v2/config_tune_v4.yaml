# Tuning v4: BiLSTM with higher dropout + learning rate adjustment
model:
  backbone: "lstm"
  pooling: "context"
  input_dim: 20
  d_model: 256
  num_layers: 2
  max_len: 200

training:
  batch_size: 32
  epochs: 20
  lr: 5e-4
  seed: 43
  limit_per_class: 2000

logging:
  save_dir: "amplify/experiments/logs"
  save_checkpoints: true

optional:
  use_esm: false
  esm_dim: null
